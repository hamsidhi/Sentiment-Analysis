# Sentiment Analysis Project - Rules & Guidelines

## ğŸ“‹ Table of Contents
1. [Project Structure Rules](#project-structure-rules)
2. [Code Quality Rules](#code-quality-rules)
3. [Data Handling Rules](#data-handling-rules)
4. [Model Training Rules](#model-training-rules)
5. [Testing & Validation Rules](#testing--validation-rules)
6. [Documentation Rules](#documentation-rules)
7. [Git & Version Control Rules](#git--version-control-rules)
8. [Performance & Optimization Rules](#performance--optimization-rules)

---

## Project Structure Rules

### Directory Organization
```
sentiment-analysis-project/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ sentiment_analyzer.py      # Main analyzer class
â”‚   â”œâ”€â”€ text_preprocessor.py       # Text cleaning & preprocessing
â”‚   â”œâ”€â”€ train_models.py            # Model training logic
â”‚   â”œâ”€â”€ predict.py                 # Prediction module
â”‚   â””â”€â”€ utils/                     # Utility functions
â”œâ”€â”€ data/                   # Data files
â”‚   â”œâ”€â”€ raw/               # Original unmodified data
â”‚   â”œâ”€â”€ processed/         # Cleaned data
â”‚   â””â”€â”€ test/              # Test datasets
â”œâ”€â”€ models/                # Trained models storage
â”œâ”€â”€ notebooks/             # Jupyter notebooks for exploration
â”œâ”€â”€ tests/                 # Unit tests
â”œâ”€â”€ examples/              # Example usage scripts
â”œâ”€â”€ config/                # Configuration files
â”œâ”€â”€ logs/                  # Log files
â”œâ”€â”€ results/               # Output results & metrics
â””â”€â”€ README.md              # Project documentation
```

### Rules
- âœ… Keep source code in `src/` directory only
- âœ… Store all data in `data/` directory, never in root or src
- âœ… Keep trained models in `models/` directory with clear naming
- âœ… All raw data goes to `data/raw/` - NEVER modify original files
- âœ… Use descriptive folder names in lowercase with underscores
- âœ… Create `__init__.py` in all Python packages
- âœ… Keep one logical module per file

---

## Code Quality Rules

### Python Standards
- âœ… Follow PEP 8 naming conventions
  - Variables & functions: `lowercase_with_underscores`
  - Classes: `PascalCase`
  - Constants: `UPPERCASE_WITH_UNDERSCORES`
- âœ… Maximum line length: 88 characters (Black formatter standard)
- âœ… Use type hints for all function parameters and returns
- âœ… Write docstrings for all classes and functions (Google style)
- âœ… Import order: Standard library â†’ Third-party â†’ Local modules
- âœ… No unused imports or variables
- âœ… Use meaningful variable names (no single letters except loops)

### Documentation
- âœ… Every function must have a docstring with:
  - Brief description
  - Parameters with types
  - Return value with type
  - Example usage (for public methods)
- âœ… Class docstrings must explain purpose and key methods
- âœ… Add comments for complex logic (WHY not WHAT)
- âœ… No commented-out code blocks - delete if not needed

### Code Organization
- âœ… Keep functions under 50 lines (split if longer)
- âœ… Keep classes focused on single responsibility
- âœ… Use private methods (underscore prefix) for internal logic
- âœ… Use constants at module level, not magic numbers
- âœ… Avoid deeply nested conditions (max 3 levels)

### Error Handling
- âœ… Raise specific exceptions, not generic ones
- âœ… Provide meaningful error messages
- âœ… Validate inputs at function entry
- âœ… Handle edge cases (empty data, None values, etc.)
- âœ… Log errors with context information
- âœ… Use try-except blocks wisely, not for flow control

---

## Data Handling Rules

### Data Integrity
- âœ… NEVER modify raw data files - create copies in `data/processed/`
- âœ… Track data source and date in metadata
- âœ… Maintain data versioning (data_v1, data_v2, etc.)
- âœ… Document all preprocessing steps
- âœ… Keep original data accessible for reproducibility

### Data Processing
- âœ… Validate data before processing (check types, ranges, nulls)
- âœ… Handle missing values explicitly (document strategy)
- âœ… Check for duplicate rows and handle appropriately
- âœ… Document data distribution before/after preprocessing
- âœ… Use consistent encoding (UTF-8 preferred)
- âœ… Preserve original text in separate column if cleaning
- âœ… Check data leakage between train/test sets

### DataFrame Requirements
- âœ… Use consistent column naming (lowercase with underscores)
- âœ… Include data type specification when loading
- âœ… Check for outliers and document treatment
- âœ… Balance datasets if needed (especially for classification)
- âœ… Include metadata about data shape and quality

### Text Data Specific
- âœ… Store raw text in separate column before cleaning
- âœ… Document all text preprocessing steps
- âœ… Track vocabulary size and changes
- âœ… Preserve case information if relevant
- âœ… Handle special characters, emojis, URLs consistently

---

## Model Training Rules

### Before Training
- âœ… Define clear objectives (accuracy targets, F1-score, etc.)
- âœ… Plan train/test split strategy before starting
- âœ… Document baseline metrics
- âœ… Set random seeds for reproducibility
  ```python
  np.random.seed(42)
  random.seed(42)
  tf.random.set_seed(42) # if using TensorFlow
  ```
- âœ… Version control all hyperparameters

### During Training
- âœ… Use stratified split for imbalanced datasets
- âœ… Log training progress and metrics
- âœ… Use consistent train/test/validation split (typically 70/15/15)
- âœ… Never train on test data
- âœ… Monitor for overfitting
- âœ… Save model checkpoints during training
- âœ… Document all model configurations

### Model Management
- âœ… Save trained models with metadata:
  - Model type and parameters
  - Training date
  - Training accuracy/metrics
  - Data version used
- âœ… Use consistent naming: `model_name_date_accuracy.pkl`
- âœ… Store models in `models/` with version numbers
- âœ… Keep model artifacts with code version tag
- âœ… Document which model version is production-ready

### Hyperparameter Tracking
- âœ… Create config file for all hyperparameters
- âœ… Document why each parameter was chosen
- âœ… Compare results across different hyperparameter sets
- âœ… Use grid search or random search systematically
- âœ… Record all hyperparameter experiments

---

## Testing & Validation Rules

### Unit Testing
- âœ… Write tests for all utility functions
- âœ… Test edge cases (empty data, None, negative numbers)
- âœ… Aim for at least 80% code coverage
- âœ… Use pytest framework
- âœ… Name test files: `test_module_name.py`
- âœ… Name test functions: `test_what_it_does()`
- âœ… Use descriptive assertion messages

### Integration Testing
- âœ… Test complete pipelines end-to-end
- âœ… Test data flow between modules
- âœ… Verify output shapes and types
- âœ… Test with different data sizes

### Model Validation
- âœ… Always use separate test set (never seen during training)
- âœ… Calculate multiple metrics: accuracy, precision, recall, F1
- âœ… Generate confusion matrix for classification
- âœ… Cross-validate with k-fold (k=5 recommended)
- âœ… Document performance on different data subsets
- âœ… Test on representative edge cases

### Validation Checklist
- âœ… Predictions make logical sense
- âœ… Model handles empty inputs gracefully
- âœ… Model handles unusual inputs without crashing
- âœ… Output format is consistent
- âœ… Performance is reproducible

---

## Documentation Rules

### README.md Requirements
- âœ… Project overview and goals
- âœ… Installation instructions
- âœ… Usage examples
- âœ… Data requirements and format
- âœ… Model performance metrics
- âœ… Troubleshooting section
- âœ… Contributing guidelines

### Code Comments
- âœ… Document WHY not WHAT
- âœ… Keep comments updated with code
- âœ… Use clear, professional language
- âœ… Avoid over-commenting obvious code

### Configuration Documentation
- âœ… Document all config parameters
- âœ… Provide default values with justification
- âœ… Explain impact of changing each parameter
- âœ… Keep config examples in README

### Results & Metrics
- âœ… Save metrics in results/ folder with date
- âœ… Include timestamp for all experiments
- âœ… Document dataset version used
- âœ… Include hyperparameters in results file
- âœ… Keep comparison table of model performance

---

## Git & Version Control Rules

### Commit Standards
- âœ… Write clear, descriptive commit messages
- âœ… Use present tense: "Add feature" not "Added feature"
- âœ… Keep commits focused on single change
- âœ… Format: `[Type] Brief description`
  - Types: `[Feature]`, `[Fix]`, `[Docs]`, `[Refactor]`, `[Test]`

### Branching Strategy
- âœ… Use feature branches: `feature/feature-name`
- âœ… Use bugfix branches: `bugfix/bug-name`
- âœ… Keep main/master branch stable and tested
- âœ… Require code review before merging to main
- âœ… Delete branches after merging

### .gitignore Rules
- âœ… Ignore data files: `data/raw/*`, `data/processed/*`
- âœ… Ignore model files: `models/*.pkl`, `models/*.h5`
- âœ… Ignore logs: `logs/*.log`
- âœ… Ignore virtual environment: `venv/`, `.venv/`
- âœ… Ignore cache: `__pycache__/`, `.pytest_cache/`
- âœ… Ignore IDE: `.vscode/`, `.idea/`
- âœ… Ignore OS files: `.DS_Store`, `Thumbs.db`

### What NOT to Commit
- âŒ Raw data files (use DVC or LFS instead)
- âŒ Trained model files (too large)
- âŒ Virtual environments
- âŒ API keys or credentials
- âŒ System/IDE files
- âŒ Large binary files

---

## Performance & Optimization Rules

### Code Performance
- âœ… Use vectorized operations (NumPy/Pandas) instead of loops
- âœ… Profile code for bottlenecks before optimizing
- âœ… Cache expensive computations
- âœ… Use generators for large datasets
- âœ… Minimize copying of large data structures

### Memory Management
- âœ… Load data in chunks if memory-constrained
- âœ… Release unused objects/connections
- âœ… Monitor memory usage during training
- âœ… Use sparse matrices for high-dimensional data

### Model Performance
- âœ… Document inference time requirements
- âœ… Test model on different hardware
- âœ… Optimize model size if needed
- âœ… Consider quantization for mobile deployment
- âœ… Benchmark against baseline models

### Logging & Monitoring
- âœ… Use logging module (not print statements)
- âœ… Set appropriate log levels: DEBUG, INFO, WARNING, ERROR
- âœ… Log important events and metrics
- âœ… Include timestamps in logs
- âœ… Rotate log files to prevent disk bloat

### Reproducibility
- âœ… Document Python and library versions
- âœ… Use requirements.txt with pinned versions
- âœ… Set random seeds at project start
- âœ… Document hardware used for training
- âœ… Provide exact preprocessing steps

---

## Checklist Before Deployment

- [ ] All tests pass
- [ ] Code follows style guidelines
- [ ] Documentation is complete and up-to-date
- [ ] Model performance meets targets
- [ ] Edge cases are handled
- [ ] Logging is in place
- [ ] Requirements.txt is updated
- [ ] No hardcoded paths or credentials
- [ ] Data pipeline is documented
- [ ] Model versioning is clear
- [ ] Performance is acceptable
- [ ] Code review completed

---

## Quick Reference: Do's and Don'ts

### DO âœ…
- Use constants for magic numbers
- Write docstrings for all functions
- Validate inputs at function entry
- Use meaningful variable names
- Keep functions small and focused
- Document your assumptions
- Test edge cases
- Version your models
- Use type hints

### DON'T âŒ
- Modify raw data files
- Hardcode file paths
- Use global variables
- Write overly complex functions
- Skip error handling
- Mix concerns in one function
- Use non-descriptive names
- Train on test data
- Ignore data leakage
- Deploy without testing

---

**Last Updated:** December 2025
**Version:** 1.0
